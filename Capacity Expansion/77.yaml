clipped_std: false
custom_softmax: false
id: 77
model:
  act_fn: ReLU
  arch_layersize: 128
  arch_n: 4
  batch_size: 256
  clip_range: 0.1
  ent_coef: 0.001
  log_std_init: 0
  lr: 0.0002
  lr_end: 0
  lr_sched: false
  use_sde: false
obs_normalizer: false
optimizer: PPO
policytype: MLP
reward_normalizer: false
starting_point: null
