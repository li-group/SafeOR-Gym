clipped_std: false
custom_softmax: false
env:
  B: 1
  D: 0.1
  L0_pen: 1
  M: 500
  P: 1
  Q: 500
  Z: 10
  alpha: 0
  beta: 0
  challenging_concentrations: false
  illeg_act_handling: prop
  max_pen_violations: 999
  maxflow: 500
  prod_cost:
  - 0.1
  - 0.1
  prod_price:
  - 1
  - 2
  reg: 1
  reg_lambda: 0
  uniform_data: false
id: 77
model:
  act_fn: ReLU
  arch_layersize: 128
  arch_n: 4
  batch_size: 256
  clip_range: 0.1
  ent_coef: 0.001
  log_std_init: 0
  lr: 0.0002
  lr_end: 0
  lr_sched: false
  use_sde: false
obs_normalizer: false
optimizer: PPO
policytype: MLP
reward_normalizer: false
starting_point: null
